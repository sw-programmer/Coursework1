{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sw-programmer/Coursework1/blob/sangwoo/CW1_Q1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiDpdqsG2EW4"
      },
      "source": [
        "#  **Coursework 1**\n",
        "\n",
        "1.   ### **Computationally Efficient Eigenfaces**\n",
        "2.   ### **Incremental PCA**\n",
        "3.   ### **PCA-LDA for Face Recognition**\n",
        "4.   ### **K-means codebook**\n",
        "5.   ### **RF classifier**\n",
        "\n",
        "\n",
        "**by 20190223\tSoohyun Ryu,\t20190247\tSangwoo Park**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvLRciu5QgA8"
      },
      "source": [
        "**1. Setting!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "DmFrSEqT2EW5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import io\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkTIgoexPBxA",
        "outputId": "2462e510-530a-4dea-a0db-38b32a028ec3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQz1q0Pdh0ZY",
        "outputId": "d321448d-9c68-4da0-bb42-97a224133fa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['__header__', '__version__', '__globals__', 'X', 'l'])\n",
            "(2576, 520)\n"
          ]
        }
      ],
      "source": [
        "#mat_file = io.loadmat(\"/content/drive/MyDrive/CV_ML/CW1/face.mat\")\n",
        "mat_file = io.loadmat(\"./face.mat\")\n",
        "mat_X = mat_file['X']\n",
        "mat_Y = mat_file['l'][0]\n",
        "\n",
        "print(mat_file.keys()) \n",
        "print(mat_X.shape)                         # 총 52명. 각 10장의 사진. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "EZ00poKQHwno",
        "outputId": "dedb4e64-96f8-411b-bc94-178db9ae9659"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(data=mat_X).transpose()\n",
        "df['label'] = mat_Y\n",
        "\n",
        "X = df.iloc[:, :-1].values\n",
        "Y = df.iloc[:, -1].values                   # column 'Target'\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, \n",
        "                                                    Y, \n",
        "                                                    test_size=0.2, \n",
        "                                                    shuffle=True,\n",
        "                                                    stratify=Y, \n",
        "                                                    random_state=1000)          \n",
        "# 얼굴 그룹 비율 유지               \n",
        "# test/train ratio = 0.2\n",
        "# random_state -> before dividing, seed for shuffling\n",
        "print('X_train shape:', X_train.shape)\n",
        "print('X_test shape:', X_test.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "np.unique(y_train, return_counts=True)    # 52명의 얼굴이 80%씩 고르게 train set에 들어간 모습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViTBzz6jUGM0"
      },
      "source": [
        "## **2. Incremental PCA**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "08Dial6zDJui"
      },
      "outputs": [],
      "source": [
        "# 평균 얼굴 계산 함수 \n",
        "def AvgCompute(X): \n",
        "  Xbar = np.mean(X, axis=0)  \n",
        "  matrix_A = X - Xbar \n",
        "  return matrix_A, Xbar\n",
        "\n",
        "# 고유값, 고유벡터 계산 함수\n",
        "def eig(S): \n",
        "  eig_vals, eig_vecs = np.linalg.eig(S) \n",
        "  sort_indices = np.argsort(eig_vals)[::-1] # 내림차순 정렬 \n",
        "  return eig_vals[sort_indices], eig_vecs[:, sort_indices]\n",
        "\n",
        "# Reconstruction에 쓰일 함수\n",
        "def reconstruct(X, PC): \n",
        "  return (X @ PC) @ PC.T\n",
        "\n",
        "# PCA 과정\n",
        "def PCA(X, num_components, low_dim = False): # X == train set, num_components == 선택할 eigen vector 개수\n",
        "  start = time.time()\n",
        "  N, D = X.shape \n",
        "  matrix_A, X_bar = AvgCompute(X)\n",
        "  if not low_dim:\n",
        "    S = (matrix_A.T @ matrix_A) / N\n",
        "  else:\n",
        "    S = (matrix_A @ matrix_A.T) / N\n",
        "\n",
        "  eig_vals, eig_vecs = eig(S)\n",
        "  principal_vals, principal_components = np.real(eig_vals[:num_components]), np.real(eig_vecs[:,:num_components]) \n",
        "  \n",
        "  if low_dim:\n",
        "    principal_components = matrix_A.T @ principal_components\n",
        "\n",
        "  reconst_X = reconstruct(matrix_A, principal_components) + X_bar \n",
        "  comp_time = time.time() - start\n",
        "  \n",
        "  return reconst_X, principal_components, comp_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Batch PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_num = 100\n",
        "\n",
        "Batch_PCA_X, Batch_PCA_pc, Batch_PCA_time = PCA(X_train, base_num, False)\n",
        "Batch_PCA_recon_error = np.linalg.norm((X_train - Batch_PCA_X), None)\n",
        "\n",
        "print(f\"Training time: {Batch_PCA_time}sec\")\n",
        "print(f\"Reconstruction error: {Batch_PCA_recon_error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Incremental PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PCA_decompose(X, num_components, low_dim = False):\n",
        "  start = time.time()\n",
        "\n",
        "  N, D = X.shape \n",
        "  matrix_A, X_bar = AvgCompute(X)\n",
        "  X_bar = np.reshape(X_bar, X_bar.shape + (1,))\n",
        "  if not low_dim:\n",
        "    S = (matrix_A.T @ matrix_A) / N\n",
        "  else:\n",
        "    S = (matrix_A @ matrix_A.T) / N\n",
        "\n",
        "  eig_vals, eig_vecs = eig(S)\n",
        "  principal_vals, principal_components = np.real(eig_vals[:num_components]), np.real(eig_vecs[:,:num_components]) \n",
        "\n",
        "  if low_dim:\n",
        "    principal_components = matrix_A.T @ principal_components\n",
        "\n",
        "  reconst_X = reconstruct(matrix_A, principal_components) + X_bar \n",
        "  comp_time = time.time() - start\n",
        "  \n",
        "  return X_bar, N, S, principal_components, comp_time, reconst_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "https://gist.github.com/iizukak/1287876/edad3c337844fac34f7e56ec09f9cb27d4907cc7# \n",
        "\n",
        "def gs_cofficient(v1, v2):\n",
        "    return np.dot(v2, v1) / np.dot(v1, v1)\n",
        "\n",
        "def multiply(cofficient, v):\n",
        "    return map((lambda x : x * cofficient), v)\n",
        "\n",
        "def proj(v1, v2):\n",
        "    return multiply(gs_cofficient(v1, v2) , v1)\n",
        "\n",
        "def gram_schmidt(X):\n",
        "    result = []\n",
        "    for i in range(len(X)):\n",
        "        temp_vec = X[i]\n",
        "        for w in result:\n",
        "            proj_vec = proj(w, X[i])\n",
        "            temp_vec = map(lambda x, y : x - y, temp_vec, proj_vec)\n",
        "        result.append(temp_vec)\n",
        "    for w in result:\n",
        "        if np.array_equal(np.zeros_like(w), w):\n",
        "            result.remove(w)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def merge_PCAs(M1, N1, S1, P1, M2, N2, S2, P2, num_components, X):\n",
        "    N3 = N1 + N2\n",
        "    M3 = (N1*M1 + N2*M2) / N3\n",
        "    S3 = (N1/N3) * S1 + (N2/N3) * S2 + (N1*N2/N3) * (M1-M2)@(M1-M2).T\n",
        "    X = []\n",
        "    for i in range(len(P1.shape[0])):\n",
        "        X.append(P1[:,i])\n",
        "    for i in range(len(P2.shape[0])):\n",
        "        X.append(P2[:,i])\n",
        "    X.append(M1-M2)\n",
        "    \n",
        "    suff_spanning = gram_schmidt(X)\n",
        "    suff_spanning = np.concatenate(suff_spanning, axis=0).T\n",
        "\n",
        "    new_mat = suff_spanning.T @ S3 @ suff_spanning\n",
        "\n",
        "    # ignore all other computations\n",
        "    start = time.time()\n",
        "    _, eig_vecs = eig(new_mat)\n",
        "    comp_time = time.time()-start\n",
        "\n",
        "    _, R = np.real(eig_vecs[:,:num_components])\n",
        "\n",
        "    P3 = suff_spanning @ R\n",
        "\n",
        "    reconst_X = reconstruct((X-M3), P3) + M3 \n",
        "\n",
        "    return M3, N3, S3, P3, comp_time, reconst_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split train dataset to 4 batches\n",
        "X_train11, X_train22, y_train11, y_train22 = train_test_split(X_train, \n",
        "                                                              y_train, \n",
        "                                                              test_size=0.5, \n",
        "                                                              shuffle=True,\n",
        "                                                              stratify=Y, \n",
        "                                                              random_state=1000) \n",
        "\n",
        "X_train1, X_train2, y_train1, y_train2 = train_test_split(X_train11, \n",
        "                                                          y_train11, \n",
        "                                                          test_size=0.5, \n",
        "                                                          shuffle=True,\n",
        "                                                          stratify=Y, \n",
        "                                                          random_state=1000)\n",
        "\n",
        "X_train3, X_train4, y_train3, y_train4 = train_test_split(X_train, \n",
        "                                                          y_train, \n",
        "                                                          test_size=0.5, \n",
        "                                                          shuffle=True,\n",
        "                                                          stratify=Y, \n",
        "                                                          random_state=1000)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train1_2 = np.concatenate((X_train1, X_train2), axis=0)\n",
        "X_train1_2_3 = np.concatenate((X_train1_2, X_train3), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "M1, N1, S1, P1, comp_time1, reconst_X1 = PCA_decompose(X_train1, base_num)\n",
        "M2, N2, S2, P2, comp_time2, reconst_X2 = PCA_decompose(X_train2, base_num)\n",
        "M3, N3, S3, P3, comp_time3, reconst_X3 = PCA_decompose(X_train3, base_num)\n",
        "M4, N4, S4, P4, comp_time4, reconst_X4 = PCA_decompose(X_train4, base_num)\n",
        "\n",
        "M5, N5, S5, P5, comp_time5, reconst_X5 = merge_PCAs(M1, N1, S1, P1, M2, N2, S2, P2, base_num, X_train1_2)\n",
        "M6, N6, S6, P6, comp_time6, reconst_X6 = merge_PCAs(M5, N5, S5, P5, M3, N3, S3, P3, base_num, X_train1_2_3)\n",
        "M7, N7, S7, P7, comp_time7, reconst_X7 = merge_PCAs(M6, N6, S6, P6, M4, N4, S4, P4, base_num, X_train)\n",
        "\n",
        "print(f\"Training time: {comp_time1+comp_time2+comp_time3+comp_time4+comp_time5+comp_time6+comp_time7}\")\n",
        "\n",
        "first_subset_PCA_recon_error = np.linalg.norm((X_train1 - reconst_X1), None)\n",
        "print(f\"Reconstruction error _ first subset: {first_subset_PCA_recon_error}\")\n",
        "Incremental_PCA_recon_error = np.linalg.norm((X_train - reconst_X7), None)\n",
        "print(f\"Reconstruction error _ first subset: {Incremental_PCA_recon_error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NN Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 너꺼 그대로 복붙했는데 변수이름 바꿔야 할듯!\n",
        "# NN Classfication\n",
        "X_result = X_test @ principal_components\n",
        "\n",
        "y_pred = []\n",
        "label = 0\n",
        "for query in X_result:\n",
        "  Args = []\n",
        "  for trained in X_Projected:\n",
        "    Args.append(np.linalg.norm(query - trained))\n",
        "  y_pred.append(y_train[np.argmin(Args)])\n",
        "\n",
        "print(accuracy_score(y_test, y_pred))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "CW1.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "d5d6455245eaf2d3a0cefd1d4e544c90a6d9a7d66e780d97de78b5ceeb836907"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit ('study': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
